{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multiclassClassification_Reuters_dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMaLUjILuEXZ46y9Mm0ggqr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aadi-mishra/Deep-Learning-Projects-Keras/blob/main/multiclassClassification_Reuters_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE0IFnvIi3Fm"
      },
      "source": [
        "The goal is to build a network to classify Reuters newswires into 46 mutually exclusive topics.Reuters dataset is a set of short newswires and their topics, published\n",
        "by Reuters in 1986. Itâ€™s a simple, widely used toy dataset for text classification. There\n",
        "are 46 different topics; some topics are more represented than others, but each topic\n",
        "has at least 10 examples in the training set.\n",
        "Reuters dataset comes packaged as part of Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txE-h_3giurS"
      },
      "source": [
        "# Load Dataset\n",
        "from keras.datasets import reuters\n",
        "from keras import models\n",
        "from keras import layers\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-NQAhVCk2Nc"
      },
      "source": [
        "The argument`num_words=10000` restricts the data to the\n",
        "10,000 most frequently occurring words found in the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leRKcCZNkjf_",
        "outputId": "c883af59-14a5-4612-fc1d-9ccee4b5241c"
      },
      "source": [
        "# Separate Training and Testing data\n",
        "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
            "2113536/2110848 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADk51bkEk7R2",
        "outputId": "b81a2a26-788d-45d8-ac97-e9a192c8c7c4"
      },
      "source": [
        "print(\"Training Data : \",len(train_data))\n",
        "print( \"Testing Data : \", len(test_data))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Data :  8982\n",
            "Testing Data :  2246\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeZarPIGljX_"
      },
      "source": [
        "Decoding data back to English words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRACumDHlFyS",
        "outputId": "ece7f1fc-e37c-4b81-ceae-0cc77a6e0456"
      },
      "source": [
        "word_index = reuters.get_word_index()\n",
        "reversed_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "decoded_newswire = ' '.join([reversed_word_index.get(i-3,'?') for i in train_data[0]])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters_word_index.json\n",
            "557056/550378 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rhaEOB6mPnY",
        "outputId": "6358dd12-abf6-402a-958c-0308138044c6"
      },
      "source": [
        "train_labels[19]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgcTR6l6mX3D"
      },
      "source": [
        "Vectorizing the Training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fve1m1VKmVLf"
      },
      "source": [
        "def vectorize_sequences(sequences, dimension = 10000):\n",
        "  results = np.zeros((len(sequences), dimension))\n",
        "  for i, sequence in enumerate(sequences):\n",
        "    results[i, sequence] = 1.\n",
        "  return results\n",
        "\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOsKKV4tnBPZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}